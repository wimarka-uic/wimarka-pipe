# MT (Machine Translation) NLP Pipeline

A comprehensive NLP pipeline for machine translation quality assessment, error detection, and text correction using state-of-the-art transformer models.

## ğŸ—ï¸ Architecture Overview

The pipeline follows a modular architecture with the following flow:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   INPUT     â”‚â”€â”€â”€â–¶â”‚  TOKENIZER   â”‚â”€â”€â”€â–¶â”‚ EMBEDDINGS   â”‚â”€â”€â”€â–¶â”‚TRANSFORMER   â”‚
â”‚   TEXT      â”‚    â”‚ (BERT-based) â”‚    â”‚(Sentence     â”‚    â”‚(DistilBERT)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚Transformers) â”‚    â”‚Error Detectionâ”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                    â”‚
                                                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OUTPUT    â”‚â—€â”€â”€â”€â”‚  CORRECTION  â”‚â—€â”€â”€â”€â”‚ EXPLANATION  â”‚â—€â”€â”€â”€â”‚MULTI-LABEL   â”‚
â”‚ CORRECTED   â”‚    â”‚(DistilBERT   â”‚    â”‚(Gemma 3)     â”‚    â”‚REGRESSION    â”‚
â”‚   TEXT      â”‚    â”‚Attention)    â”‚    â”‚Explanation   â”‚    â”‚(Scoring)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Pipeline Flow

### 1. **Input Processing**
- Raw text input validation and preprocessing
- Text normalization and cleaning

### 2. **Tokenization** 
- BERT-based tokenizer for text segmentation
- Handles special tokens and subword tokenization
- Configurable max length and truncation

### 3. **Embedding Generation**
- Sentence Transformers for semantic embeddings
- Generates high-dimensional vector representations
- Supports batch processing for efficiency

### 4. **Error Detection (DistilBERT)**
- Advanced error detection using DistilBERT model
- Identifies grammatical, spelling, and semantic errors
- Provides confidence scores and suggestions
- Classification-based error detection with attention mechanism

### 5. **Multi-Label Regression Scoring**
- DistilBERT-based scoring across multiple dimensions:
  - Grammar Quality
  - Spelling Accuracy
  - Semantic Coherence
  - Fluency
  - Clarity
  - Overall Quality
- Error-aware score adjustment

### 6. **Explanation Generation (Gemma 3)**
- Advanced explanation generation using Google's Gemma 3 model
- Generates human-readable explanations for errors
- Provides improvement suggestions
- Context-aware explanations with high-quality text generation

### 7. **Correction Generation (DistilBERT Attention)**
- Attention-based correction using DistilBERT
- Focuses on error regions using attention weights
- Generates corrected text versions
- Maintains semantic coherence

## ğŸ“ Project Structure

```
wimarka-pipe/
â”œâ”€â”€ main.py                          # Main entry point
â”œâ”€â”€ requirements.txt                 # Dependencies
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ pipeline/                       # Main pipeline package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                   # Configuration management
â”‚   â”œâ”€â”€ pipeline.py                 # Main pipeline orchestrator
â”‚   â”œâ”€â”€ components/                 # Pipeline components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tokenizer.py           # Tokenization component
â”‚   â”‚   â”œâ”€â”€ embeddings.py          # Embedding generation
â”‚   â”‚   â”œâ”€â”€ error_detection.py     # Error detection (Gemma 3)
â”‚   â”‚   â”œâ”€â”€ scoring.py             # Multi-label scoring
â”‚   â”‚   â”œâ”€â”€ explanation.py         # Explanation generation (T5)
â”‚   â”‚   â””â”€â”€ correction.py          # Correction generation (DistilBERT)
â”‚   â””â”€â”€ utils/                     # Utility modules
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ cache.py               # Caching system
â”‚       â””â”€â”€ logger.py              # Logging utilities
â”œâ”€â”€ cache/                         # Model cache directory
â””â”€â”€ output/                        # Output directory
```

## ğŸš€ Quick Start

### Installation

1. **Clone the repository:**
```bash
git clone <repository-url>
cd wimarka-pipe
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Run the pipeline:**
```bash
python main.py
```

### Basic Usage

```python
from pipeline.config import PipelineConfig
from pipeline.pipeline import MTPipeline

# Initialize configuration
config = PipelineConfig()

# Create pipeline instance
pipeline = MTPipeline(config)

# Process text
input_text = "This is a sample text with some errors."
result = pipeline.process(input_text)

# Access results
print(f"Input: {result.input_text}")
print(f"Errors detected: {len(result.error_detections)}")
print(f"Quality scores: {result.scores}")
print(f"Explanations: {result.explanations}")
print(f"Corrections: {result.corrections}")
print(f"Final output: {result.final_output}")
```

## âš™ï¸ Configuration

The pipeline is highly configurable through the `PipelineConfig` class:

```python
from pipeline.config import PipelineConfig, ModelConfig

# Custom configuration
config = PipelineConfig(
    tokenizer_config=ModelConfig(
        model_name="bert-base-uncased",
        max_length=512
    ),
    error_detection_config=ModelConfig(
        model_name="google/gemma-3-2b",
        max_length=512
    ),
    # ... other configurations
)
```

### Key Configuration Options

- **Model Selection**: Choose different models for each component
- **Device Management**: Auto-detect or specify CPU/GPU usage
- **Caching**: Enable/disable result caching for performance
- **Batch Processing**: Configure batch sizes for efficiency
- **Logging**: Set log levels and output destinations

## ğŸ”§ Components Details

### Tokenizer Component
- **Model**: BERT-based tokenizer
- **Features**: Subword tokenization, special token handling
- **Configurable**: Max length, truncation, padding

### Embedding Generator
- **Model**: Sentence Transformers
- **Features**: Semantic embeddings, batch processing
- **Output**: High-dimensional vectors

### Error Detector (DistilBERT)
- **Model**: DistilBERT with custom error detection head
- **Features**: Classification-based error detection
- **Output**: Error classifications with confidence scores

### Multi-Label Scorer
- **Model**: DistilBERT with custom scoring head
- **Features**: 6-dimensional quality scoring
- **Output**: Normalized scores (0-1) for each dimension

### Explanation Generator (Gemma 3)
- **Model**: Google Gemma 3 (2B parameters)
- **Features**: Advanced text generation for explanations
- **Output**: High-quality human-readable explanations and suggestions

### Correction Generator (DistilBERT Attention)
- **Model**: DistilBERT with attention mechanism
- **Features**: Attention-weighted corrections
- **Output**: Corrected text versions


## ğŸ™ Acknowledgments

- **Google**: Gemma 3 model
- **Hugging Face**: Transformers library and DistilBERT
- **Sentence Transformers**: Embedding models
- **DistilBERT**: Efficient BERT variant for error detection

---

**Note**: This pipeline is designed for research and educational purposes. For production use, additional testing, validation, and optimization may be required.
